{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36337ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages to download, you only need to run this once!\n",
    "!pip3 install google.generativeai\n",
    "!pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T19:37:54.077565Z",
     "start_time": "2025-04-01T19:37:53.090082Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import re\n",
    "import time\n",
    "\n",
    "# TODO: put your Google API key\n",
    "api_key = 'AIzaSyBK375J_WOHHnxjTY8_PnN2rUIj48KoBjY'  # TODO put your api key\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "\n",
    "def call_google_api(prompt, my_model):\n",
    "    \"\"\"\n",
    "    Method for getting a response from the Gemini API.\n",
    "    Args:\n",
    "        - prompt (str): The input instruction for the language model.\n",
    "        - my_model: The Gemini model instance.\n",
    "    Returns:\n",
    "        str: The generated response, or None if no response is available.\n",
    "    \"\"\"\n",
    "    google_model_config = genai.types.GenerationConfig(temperature=0, max_output_tokens=6000)\n",
    "    completion = my_model.generate_content(prompt, generation_config=google_model_config)\n",
    "    try:\n",
    "        gemini_response_text = completion.text\n",
    "    except Exception as e:\n",
    "        print(\"Gemini response error: \" + str(e))\n",
    "        try:\n",
    "            if hasattr(completion.parts, 'text'):\n",
    "                gemini_response_text = completion.parts.text\n",
    "            else:\n",
    "                gemini_response_text = None\n",
    "        except Exception:\n",
    "            gemini_response_text = None\n",
    "    return gemini_response_text\n",
    "\n",
    "def clean_generated_code(generated_code, language):\n",
    "    \"\"\"\n",
    "    Helper method for cleaning LLM-generated code.\n",
    "    Args:\n",
    "        - generated_code (str): The raw code output from the LLM.\n",
    "        - language (str): A string indicating the language of the code (e.g., \"python3\").\n",
    "    Returns:\n",
    "        str: Cleaned LLM-generated code.\n",
    "    \"\"\"\n",
    "    if not generated_code:\n",
    "        return \"\"\n",
    "    \n",
    "    code = re.sub(r\"(def[^\\n]+:\\s*)('''[\\s\\S]*?''')\", r\"\\1\", generated_code)\n",
    "    code = re.sub(r'(def[^\\n]+:\\s*)(\"\"\"[\\s\\S]*?\"\"\")', r\"\\1\", code)\n",
    "    code = re.sub(r\"(def[^\\n]+:\\n)\\s*\\n\", r\"\\1\", code)\n",
    "    \n",
    "    cleaned_code = []\n",
    "    for line in code.split('\\n'):\n",
    "        if f\"```{language}\" in line or line.strip().startswith(\"```\"):\n",
    "            continue\n",
    "        cleaned_code.append(line)\n",
    "    return \"\\n\".join(cleaned_code)\n",
    "\n",
    "def get_llm_response(prompt):\n",
    "    \"\"\"\n",
    "    Wrapper method for retrieving and cleaning LLM-generated code using the two functions above.\n",
    "    1. call_google_api: Gets a response from the Gemini 1.5 model via the Google API.\n",
    "    2. clean_generated_code: Cleans the generated code by removing code block markers.\n",
    "    Args:\n",
    "        - prompt (str): The code generation prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned code if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    # Maximum 5 attempts (this number can be adjusted as needed).\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            res = call_google_api(prompt, model)\n",
    "            return clean_generated_code(res, 'python3')\n",
    "        except Exception as e:\n",
    "            time.sleep(5)\n",
    "        if res is None or res.lower() == 'none':\n",
    "            print(f\"llm did not respond for problem\")\n",
    "    return None\n",
    "\n",
    "# Do NOT change this prompt template\n",
    "CODE_GENERATION_PROMPT_TEMPLATE = \"\"\"\n",
    "System:\n",
    "## Persona\n",
    "- You are a code generation assistant who specializes in {language}.\n",
    "- You follow strict guidelines for producing high-quality, readable, and correct code.\n",
    "\n",
    "## Instructions\n",
    "- You will be given a coding question specification, which consists of function signatures, and docstrings.\n",
    "- Your task is to **generate the complete, correct {language} code** based on the provided docstring and requirements.\n",
    "- You must think step by step when generating the {language} code.\n",
    "\n",
    "## Output Format\n",
    "- Your **final code** should be enclosed in a code block, for example:\n",
    "  ```{language}\n",
    "  # your code here\n",
    "- Do not add additional text or commentary outside of the code block.\n",
    "\n",
    "User:\n",
    "### Coding Question Specification\n",
    "{problem_stmt}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9ec4f9b9449590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T19:38:02.888582Z",
     "start_time": "2025-04-01T19:38:02.886225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep the function name as: order_by_points\n",
    "\n",
    "original_stmt = \"\"\"def order_by_points(nums):\n",
    "    '''\n",
    "    Write a function which sorts the given list of integers\n",
    "    in ascending order according to the sum of their digits.\n",
    "    Note: if there are several items with similar sum of their digits,\n",
    "    order them based on their index in original list.\n",
    "\n",
    "    Examples:\n",
    "    >>> order_by_points([1, 11, -1, -11, -12]) == [-1, -11, 1, -12, 11]\n",
    "    >>> order_by_points([]) == []\n",
    "    '''\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86624b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def order_by_points(nums):\n",
      "    if not nums:\n",
      "        return []\n",
      "\n",
      "    def sum_digits(n):\n",
      "        s = 0\n",
      "        n = abs(n)\n",
      "        while n:\n",
      "            s += n % 10\n",
      "            n //= 10\n",
      "        return s\n",
      "\n",
      "    with_indices = list(enumerate(nums))\n",
      "    with_indices.sort(key=lambda x: (sum_digits(x[1]), x[0]))\n",
      "    return [x[1] for x in with_indices]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate code using the Gemini model. Here we build the prompt using the `original_stmt`\"\"\"\n",
    "\n",
    "# Specify the function name and Canvas group number\n",
    "function_name = 'order_by_points'\n",
    "canvas_group_number = '83'       #TODO:you can change this to your Canvas group name\n",
    "\n",
    "# LLM-generated code using original_stmt\n",
    "my_prompt = CODE_GENERATION_PROMPT_TEMPLATE.format(problem_stmt=original_stmt, language=\"python3\")\n",
    "llm_code_original = get_llm_response(my_prompt)\n",
    "print(llm_code_original)\n",
    "\n",
    "# Save the Python file for testing\n",
    "filename_original = f\"hw5-{function_name}-group{canvas_group_number}-original.py\"\n",
    "with open(filename_original, \"w\") as file:\n",
    "    file.write(llm_code_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "206073fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 failed: input=[1, 11, -1, -11, -12], expected=[-1, -11, 1, -12, 11], got=[1, -1, 11, -11, -12]\n",
      "Test case 2 passed.\n",
      "Test case 3 passed.\n",
      "Test case 4 passed.\n",
      "Test case 5 passed.\n",
      "Test case 6 passed.\n",
      "Test case 7 passed.\n",
      "Test case 8 passed.\n",
      "Test case 9 passed.\n",
      "Test case 10 passed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib.util\n",
    "# Load the JSON file for the test cases\n",
    "with open(f'test_case_{function_name}.json', 'r') as f:\n",
    "    test_cases = json.load(f)[\"test_case_improved\"]\n",
    "\n",
    "# Load the saved Python file using function name and file_name\n",
    "spec = importlib.util.spec_from_file_location(function_name, filename_original)\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "function = getattr(module, function_name)\n",
    "\n",
    "# Run test cases\n",
    "for idx, case in enumerate(test_cases):\n",
    "    try:\n",
    "        inputs = case[\"input\"]\n",
    "        if isinstance(inputs, (list, tuple)):\n",
    "            try:\n",
    "                result = function(*inputs)\n",
    "            except TypeError:\n",
    "                result = function(inputs)\n",
    "        else:\n",
    "            result = function(inputs)\n",
    "\n",
    "        assert result == case[\"expected\"], f\"Test {idx+1} failed: input={inputs}, expected={case['expected']}, got={result}\"\n",
    "        print(f\"Test case {idx+1} passed.\")\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2b5b6fc3ce263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T19:38:13.774246Z",
     "start_time": "2025-04-01T19:38:13.771878Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Generate code using `original_stmt`.\n",
    "# 2. Write test cases (in the provided JSON file) to evaluate the LLM-generated code based on `original_stmt`.\n",
    "# 3. Run your test cases and demonstrate examples of both failing and passing cases.\n",
    "#    (You do not have to follow the exact implementation shown in the demo_same_chars file, but you are welcome to reference or reuse parts of it.)\n",
    "# 4. Write a `new_stmt` that improves the prompt to enhance the accuracy of LLM-generated code.\n",
    "# 5. Generate code using `new_stmt` and run your test cases, ensuring that the code passes all of them.\n",
    "# 6. Ensure you write as many test cases as needed to cover all edge cases.\n",
    "#    We will run the autograder against your final LLM-generated code (using `new_stmt`), ensure that it can pass all autograder test cases.\n",
    "\n",
    "new_stmt = \"\"\"def order_by_points(nums):\n",
    "    '''\n",
    "    Write a function which sorts the given list of integers\n",
    "    in ascending order according to the sum of their digits.\n",
    "    For each number in the list nums, calculate their given sum based on the sum of digits template that I will be providing below.\n",
    "    Negative numbers should only count the first digit of the number as negative when calculating the sum.\n",
    "    You may write a helper function to calculate the sum of digits of a given integer.\n",
    "    For calculating the sum of digits, you may turn the given integer into a string first for purposes of parsing, but your helper MUST return an integer.\n",
    "    Note: if there are several items with similar sum of their digits,\n",
    "    order them based on their index in original list.\n",
    "\n",
    "    Examples of sum of digits:\n",
    "    11 == 2\n",
    "    -11 == -1 + 1 == 0\n",
    "\n",
    "    \n",
    "\n",
    "    Examples:\n",
    "    >>> order_by_points([1, 11, -1, -11, -12]) == [-1, -11, 1, -12, 11]\n",
    "    >>> order_by_points([]) == []\n",
    "    >>> order_by_points([7]) == [7]\n",
    "    >>> order_by_points([0, 1, 100, 10]) == [0, 1, 100, 10]\n",
    "    >>> order_by_points([15, 6, 12, 3, 9]) == [12, 3, 15, 6, 9]\n",
    "    >>> order_by_points([[0, -0, 1, -1]]) == [-1, 0, -0, 1]\n",
    "\n",
    "    '''\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43e945a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def sum_digits(n):\n",
      "    s = 0\n",
      "    sign = -1 if n < 0 else 1\n",
      "    n = abs(n)\n",
      "    s += sign * (n // (10 ** (len(str(n)) - 1)))\n",
      "    n %= (10 ** (len(str(n)) - 1))\n",
      "    while n > 0:\n",
      "        s += n % 10\n",
      "        n //= 10\n",
      "    return s\n",
      "\n",
      "def order_by_points(nums):\n",
      "    if not nums:\n",
      "        return []\n",
      "    \n",
      "    with_sums = []\n",
      "    for i, num in enumerate(nums):\n",
      "        with_sums.append((sum_digits(num), i, num))\n",
      "    \n",
      "    with_sums.sort()\n",
      "    \n",
      "    result = [num for _, _, num in with_sums]\n",
      "    return result\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate code using the Gemini model. Here we build the prompt using the `original_stmt`\"\"\"\n",
    "\n",
    "# Specify the function name and Canvas group number\n",
    "function_name = 'order_by_points'\n",
    "canvas_group_number = '83'       #TODO:you can change this to your Canvas group name\n",
    "\n",
    "# LLM-generated code using original_stmt\n",
    "my_prompt = CODE_GENERATION_PROMPT_TEMPLATE.format(problem_stmt=new_stmt, language=\"python3\")\n",
    "llm_code_improved = get_llm_response(my_prompt)\n",
    "print(llm_code_improved)\n",
    "\n",
    "# Save the Python file for testing\n",
    "filename_improved = f\"hw5-{function_name}-group{canvas_group_number}-improved.py\"\n",
    "with open(filename_improved, \"w\") as file:\n",
    "    file.write(llm_code_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37b4384e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case 1 passed.\n",
      "Test case 2 passed.\n",
      "Test case 3 passed.\n",
      "Test case 4 passed.\n",
      "Test case 5 passed.\n",
      "Test case 6 passed.\n",
      "Test case 7 passed.\n",
      "Test case 8 passed.\n",
      "Test case 9 passed.\n",
      "Test case 10 passed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib.util\n",
    "# Load the JSON file for the test cases\n",
    "with open(f'test_case_{function_name}.json', 'r') as f:\n",
    "    test_cases = json.load(f)[\"test_case_improved\"]\n",
    "\n",
    "# Load the saved Python file using function name and file_name\n",
    "spec = importlib.util.spec_from_file_location(function_name, filename_improved)\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "function = getattr(module, function_name)\n",
    "\n",
    "# Run test cases\n",
    "for idx, case in enumerate(test_cases):\n",
    "    try:\n",
    "        inputs = case[\"input\"]\n",
    "        if isinstance(inputs, (list, tuple)):\n",
    "            try:\n",
    "                result = function(*inputs)\n",
    "            except TypeError:\n",
    "                result = function(inputs)\n",
    "        else:\n",
    "            result = function(inputs)\n",
    "\n",
    "        assert result == case[\"expected\"], f\"Test {idx+1} failed: input={inputs}, expected={case['expected']}, got={result}\"\n",
    "        print(f\"Test case {idx+1} passed.\")\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
