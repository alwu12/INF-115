{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages to download, you only need to run this once!\n",
    "!pip3 install google.generativeai\n",
    "!pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T19:36:27.330261Z",
     "start_time": "2025-04-01T19:36:26.490199Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import re\n",
    "import time\n",
    "\n",
    "# TODO: put your Google API key\n",
    "api_key = 'AIzaSyBK375J_WOHHnxjTY8_PnN2rUIj48KoBjY'  # TODO put your api key\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "\n",
    "def call_google_api(prompt, my_model):\n",
    "    \"\"\"\n",
    "    Method for getting a response from the Gemini API.\n",
    "    Args:\n",
    "        - prompt (str): The input instruction for the language model.\n",
    "        - my_model: The Gemini model instance.\n",
    "    Returns:\n",
    "        str: The generated response, or None if no response is available.\n",
    "    \"\"\"\n",
    "    google_model_config = genai.types.GenerationConfig(temperature=0, max_output_tokens=6000)\n",
    "    completion = my_model.generate_content(prompt, generation_config=google_model_config)\n",
    "    try:\n",
    "        gemini_response_text = completion.text\n",
    "    except Exception as e:\n",
    "        print(\"Gemini response error: \" + str(e))\n",
    "        try:\n",
    "            if hasattr(completion.parts, 'text'):\n",
    "                gemini_response_text = completion.parts.text\n",
    "            else:\n",
    "                gemini_response_text = None\n",
    "        except Exception:\n",
    "            gemini_response_text = None\n",
    "    return gemini_response_text\n",
    "\n",
    "def clean_generated_code(generated_code, language):\n",
    "    \"\"\"\n",
    "    Helper method for cleaning LLM-generated code.\n",
    "    Args:\n",
    "        - generated_code (str): The raw code output from the LLM.\n",
    "        - language (str): A string indicating the language of the code (e.g., \"python3\").\n",
    "    Returns:\n",
    "        str: Cleaned LLM-generated code.\n",
    "    \"\"\"\n",
    "    if not generated_code:\n",
    "        return \"\"\n",
    "    \n",
    "    code = re.sub(r\"(def[^\\n]+:\\s*)('''[\\s\\S]*?''')\", r\"\\1\", generated_code)\n",
    "    code = re.sub(r'(def[^\\n]+:\\s*)(\"\"\"[\\s\\S]*?\"\"\")', r\"\\1\", code)\n",
    "    code = re.sub(r\"(def[^\\n]+:\\n)\\s*\\n\", r\"\\1\", code)\n",
    "    \n",
    "    cleaned_code = []\n",
    "    for line in code.split('\\n'):\n",
    "        if f\"```{language}\" in line or line.strip().startswith(\"```\"):\n",
    "            continue\n",
    "        cleaned_code.append(line)\n",
    "    return \"\\n\".join(cleaned_code)\n",
    "\n",
    "def get_llm_response(prompt):\n",
    "    \"\"\"\n",
    "    Wrapper method for retrieving and cleaning LLM-generated code using the two functions above.\n",
    "    1. call_google_api: Gets a response from the Gemini 1.5 model via the Google API.\n",
    "    2. clean_generated_code: Cleans the generated code by removing code block markers.\n",
    "    Args:\n",
    "        - prompt (str): The code generation prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned code if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    # Maximum 5 attempts (this number can be adjusted as needed).\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            res = call_google_api(prompt, model)\n",
    "            return clean_generated_code(res, 'python3')\n",
    "        except Exception as e:\n",
    "            time.sleep(5)\n",
    "        if res is None or res.lower() == 'none':\n",
    "            print(f\"llm did not respond for problem\")\n",
    "    return None\n",
    "\n",
    "# Do NOT change this prompt template\n",
    "CODE_GENERATION_PROMPT_TEMPLATE = \"\"\"\n",
    "System:\n",
    "## Persona\n",
    "- You are a code generation assistant who specializes in {language}.\n",
    "- You follow strict guidelines for producing high-quality, readable, and correct code.\n",
    "\n",
    "## Instructions\n",
    "- You will be given a coding question specification, which consists of function signatures, and docstrings.\n",
    "- Your task is to **generate the complete, correct {language} code** based on the provided docstring and requirements.\n",
    "- You must think step by step when generating the {language} code.\n",
    "\n",
    "## Output Format\n",
    "- Your **final code** should be enclosed in a code block, for example:\n",
    "  ```{language}\n",
    "  # your code here\n",
    "- Do not add additional text or commentary outside of the code block.\n",
    "\n",
    "User:\n",
    "### Coding Question Specification\n",
    "{problem_stmt}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9ec4f9b9449590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T19:36:31.735536Z",
     "start_time": "2025-04-01T19:36:31.733032Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Example prompt for instructing an LLM to generate a function named `same_chars`. Describes the intended functionality and expected behavior of the code.\"\"\"\n",
    "\n",
    "original_stmt = '''def same_chars(s0: str, s1: str):\n",
    "    \"\"\"\n",
    "    Check if the words have the same characters.\n",
    "    >>> same_chars('eabcdzzzz', 'dddzzzzzzzddeddabc')\n",
    "    True\n",
    "    >>> same_chars('abcd', 'dddddddabc')\n",
    "    True\n",
    "    >>> same_chars('dddddddabc', 'abcd')\n",
    "    True\n",
    "    >>> same_chars('eabcd', 'dddddddabc')\n",
    "    False\n",
    "    >>> same_chars('abcd', 'dddddddabce')\n",
    "    False\n",
    "    >>> same_chars('eabcdzzzz', 'dddzzzzzzzddddabc')\n",
    "    False\n",
    "    \"\"\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf995c78ac2d69e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T19:36:44.271681Z",
     "start_time": "2025-04-01T19:36:41.503476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def same_chars(s0: str, s1: str):\n",
      "    dict0 = {}\n",
      "    dict1 = {}\n",
      "    for char in s0:\n",
      "        if char in dict0:\n",
      "            dict0[char] += 1\n",
      "        else:\n",
      "            dict0[char] = 1\n",
      "    for char in s1:\n",
      "        if char in dict1:\n",
      "            dict1[char] += 1\n",
      "        else:\n",
      "            dict1[char] = 1\n",
      "    for key in dict0:\n",
      "        if key not in dict1 or dict0[key] != dict1[key]:\n",
      "            return False\n",
      "    for key in dict1:\n",
      "        if key not in dict0 or dict1[key] != dict0[key]:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate code using the Gemini model. Here we build the prompt using the `original_stmt`\"\"\"\n",
    "\n",
    "# Specify the function name and Canvas group number\n",
    "function_name = 'same_chars'\n",
    "canvas_group_number = '1'       #TODO:you can change this to your Canvas group name\n",
    "\n",
    "# LLM-generated code using original_stmt\n",
    "my_prompt = CODE_GENERATION_PROMPT_TEMPLATE.format(problem_stmt=original_stmt, language=\"python3\")\n",
    "llm_code_original = get_llm_response(my_prompt)\n",
    "print(llm_code_original)\n",
    "\n",
    "# Save the Python file for testing\n",
    "filename_original = f\"hw5-{function_name}-group{canvas_group_number}-original.py\"\n",
    "with open(filename_original, \"w\") as file:\n",
    "    file.write(llm_code_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75002a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 failed: input=['eabcdzzzz', 'dddzzzzzzzddeddabc'], expected=True, got=False\n",
      "Test 2 failed: input=['abcd', 'dddddddabc'], expected=True, got=False\n",
      "Test 3 failed: input=['dddddddabc', 'abcd'], expected=True, got=False\n",
      "Test case 4 passed.\n",
      "Test case 5 passed.\n",
      "Test case 6 passed.\n",
      "Test case 7 passed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example of running test cases on LLM-generated code. You do not need to follow this exact implementation for your code.\"\"\"\n",
    "\n",
    "import json\n",
    "import importlib.util\n",
    "\n",
    "# Load the JSON file for the test cases\n",
    "with open(f'test_case_{function_name}.json', 'r') as f:\n",
    "    test_cases = json.load(f)[\"test_case\"]\n",
    "\n",
    "# Load the saved Python file using function name and file_name\n",
    "spec = importlib.util.spec_from_file_location(function_name, filename_original)\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "function = getattr(module, function_name)\n",
    "\n",
    "# Run test cases\n",
    "for idx, case in enumerate(test_cases):\n",
    "    try:\n",
    "        inputs = case[\"input\"]\n",
    "        if isinstance(inputs, (list, tuple)):\n",
    "            try:\n",
    "                result = function(*inputs)\n",
    "            except TypeError:\n",
    "                result = function(inputs)\n",
    "        else:\n",
    "            result = function(inputs)\n",
    "\n",
    "        assert result == case[\"expected\"], f\"Test {idx+1} failed: input={inputs}, expected={case['expected']}, got={result}\"\n",
    "        print(f\"Test case {idx+1} passed.\")\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b2b5b6fc3ce263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T19:36:53.409846Z",
     "start_time": "2025-04-01T19:36:53.407768Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Example of modifying a prompt to improve LLM-generated code. Compare with `original_stmt` to see what has been added or changed.\"\"\"\n",
    "\n",
    "new_stmt = '''def same_chars(s0: str, s1: str):\n",
    "    \"\"\"\n",
    "    Check if two words have the same set of unique characters.\n",
    "    >>> same_chars('eabcdzzzz', 'dddzzzzzzzddeddabc')\n",
    "    True\n",
    "    >>> same_chars('abcd', 'dddddddabc')\n",
    "    True\n",
    "    >>> same_chars('dddddddabc', 'abcd')\n",
    "    True\n",
    "    >>> same_chars('eabcd', 'dddddddabc')\n",
    "    False\n",
    "    >>> same_chars('abcd', 'dddddddabce')\n",
    "    False\n",
    "    >>> same_chars('eabcdzzzz', 'dddzzzzzzzddddabc')\n",
    "    False\n",
    "    \"\"\"'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b9aaae7e2ae2bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T19:36:58.284799Z",
     "start_time": "2025-04-01T19:36:55.989799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def same_chars(s0: str, s1: str):\n",
      "    set0 = set(s0)\n",
      "    set1 = set(s1)\n",
      "    return set0 == set1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate code using the Gemini model. Here we build the prompt using the `new_stmt`\"\"\"\n",
    "\n",
    "# LLM-generated code using new_stmt\n",
    "my_prompt = CODE_GENERATION_PROMPT_TEMPLATE.format(problem_stmt=new_stmt, language=\"python3\")\n",
    "llm_code_improved = get_llm_response(my_prompt)\n",
    "print(llm_code_improved)\n",
    "\n",
    "# Save the Python file for testing\n",
    "filename_improved = f\"hw5-{function_name}-group{canvas_group_number}-improved.py\"\n",
    "with open(filename_improved, \"w\") as file:\n",
    "    file.write(llm_code_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5582912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case 1 passed.\n",
      "Test case 2 passed.\n",
      "Test case 3 passed.\n",
      "Test case 4 passed.\n",
      "Test case 5 passed.\n",
      "Test case 6 passed.\n",
      "Test case 7 passed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Running test cases for the LLM-generated code using the modified prompt\"\"\"\n",
    "\n",
    "import json\n",
    "import importlib.util\n",
    "\n",
    "# Load the JSON file for the test cases\n",
    "with open(f'test_case_{function_name}.json', 'r') as f:\n",
    "    test_cases = json.load(f)[\"test_case\"]\n",
    "\n",
    "# Load the saved Python file using function name and file_name\n",
    "spec = importlib.util.spec_from_file_location(function_name, filename_improved)\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)\n",
    "function = getattr(module, function_name)\n",
    "\n",
    "# Run test cases\n",
    "for idx, case in enumerate(test_cases):\n",
    "    try:\n",
    "        inputs = case[\"input\"]\n",
    "        if isinstance(inputs, (list, tuple)):\n",
    "            try:\n",
    "                result = function(*inputs)\n",
    "            except TypeError:\n",
    "                result = function(inputs)\n",
    "        else:\n",
    "            result = function(inputs)\n",
    "        assert result == case[\"expected\"], f\"Test {idx+1} failed: input={inputs}, expected={case['expected']}, got={result}\"\n",
    "        print(f\"Test case {idx+1} passed.\")\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
